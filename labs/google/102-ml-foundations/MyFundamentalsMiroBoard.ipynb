{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "s# Class 102 at Gervi-Hera-Vitr\n",
    "\n",
    "We are learning about machine learning from Google!\n",
    "\n",
    "- Here is the [Machine Learning Foundations YouTube Class](https://youtube.com/playlist?list=PLOU2XLYxmsII9mzQ-Xxug4l2o04JBrkLV&si=13sb6VTquza6cmUA\n",
    ") that we follow.\n",
    "\n",
    "- And here is the [Captain Lugaru's Assignment Miro Board](https://miro.com/app/board/uXjVL-vk0o4=/ \"Episodes Explained\")"
   ],
   "id": "4c4566e2d30852ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " Episode 1: Basics of ML.\n",
    " Ok, so what is ML?:\n",
    " Machine learning is a branch of Ai that enables computer systems to learn and improve from experience without being explicitly programmed. It focuses on creating algorithms and models that can analyze data, identify patterns, and make decisions or predictions based on that data. Ml is not Like traditional programming:\n",
    " ![](./rules-data-logic-answers.png)"
   ],
   "id": "fbbd25d0674e0a74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ok, so rules + data goes into traditional programming give you answers, but with Machine learning you put in data and answers to get out perfect rules, so that later you can have the model predict the other answers much better and faster than traditional programming using these new rules, ok? well now we are going to look at how a neural network works down below:",
   "id": "63b7289987754295"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](layers.png)\n",
   "id": "3a2ad8b528b6de0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A group of Neurons that are side by side is called a layer, and you can have multiple layers that all communicate with each other and eventually coming up with a prediction or solution. but now that you understand the very basics of ML we will start Class 1 Coding aka the \"hello world\" of ML!:\n",
   "id": "53ec3d8ae9d9d7a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T00:55:46.159998Z",
     "start_time": "2024-12-09T00:55:36.463738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras # imports\n",
    "import numpy as np\n",
    "model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) # This line creates a machine learning model using Keras, structured as a sequence of layers. It has a single dense (fully connected) layer with 1 unit (neuron) and expects input data with one feature (dimension).\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error') # This line sets up the model to learn by using the Stochastic Gradient Descent (SGD) optimizer to update its weights. It uses mean squared error to measure how far predictions are from the actual values during training.\n",
    "\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) # data used for x and y arrays\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "model.fit(xs, ys, epochs=200)#model is fitted and trained on the x and y and is given a number of epochs\n",
    "\n",
    "outcome = model.predict(np.array([4.0]))# model predicts what an x of 4 will mean for y\n",
    "print(outcome.round()) # should predict the right answer: 7.\n"
   ],
   "id": "bdb34a8ad01b63ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/miniforge3/envs/ml/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 378ms/step - loss: 18.6934\n",
      "Epoch 2/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 14.9668\n",
      "Epoch 3/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 12.0295\n",
      "Epoch 4/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 9.7134\n",
      "Epoch 5/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 7.8861\n",
      "Epoch 6/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 6.4434\n",
      "Epoch 7/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 5.3035\n",
      "Epoch 8/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 4.4018\n",
      "Epoch 9/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 3.6878\n",
      "Epoch 10/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 3.1213\n",
      "Epoch 11/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 2.6712\n",
      "Epoch 12/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 2.3126\n",
      "Epoch 13/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 2.0262\n",
      "Epoch 14/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 1.7965\n",
      "Epoch 15/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step - loss: 1.6117\n",
      "Epoch 16/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 1.4623\n",
      "Epoch 17/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 1.3407\n",
      "Epoch 18/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 49ms/step - loss: 1.2411\n",
      "Epoch 19/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 1.1590\n",
      "Epoch 20/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 1.0906\n",
      "Epoch 21/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 1.0331\n",
      "Epoch 22/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 51ms/step - loss: 0.9843\n",
      "Epoch 23/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 55ms/step - loss: 0.9423\n",
      "Epoch 24/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 55ms/step - loss: 0.9059\n",
      "Epoch 25/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step - loss: 0.8739\n",
      "Epoch 26/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 60ms/step - loss: 0.8453\n",
      "Epoch 27/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step - loss: 0.8197\n",
      "Epoch 28/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 52ms/step - loss: 0.7963\n",
      "Epoch 29/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 0.7748\n",
      "Epoch 30/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.7548\n",
      "Epoch 31/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.7361\n",
      "Epoch 32/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.7185\n",
      "Epoch 33/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 55ms/step - loss: 0.7018\n",
      "Epoch 34/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 50ms/step - loss: 0.6858\n",
      "Epoch 35/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.6705\n",
      "Epoch 36/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 0.6557\n",
      "Epoch 37/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.6415\n",
      "Epoch 38/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 57ms/step - loss: 0.6278\n",
      "Epoch 39/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.6144\n",
      "Epoch 40/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 0.6014\n",
      "Epoch 41/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.5888\n",
      "Epoch 42/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.5764\n",
      "Epoch 43/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 49ms/step - loss: 0.5644\n",
      "Epoch 44/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 52ms/step - loss: 0.5527\n",
      "Epoch 45/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.5412\n",
      "Epoch 46/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.5300\n",
      "Epoch 47/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 0.5191\n",
      "Epoch 48/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.5083\n",
      "Epoch 49/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.4979\n",
      "Epoch 50/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.4876\n",
      "Epoch 51/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.4776\n",
      "Epoch 52/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - loss: 0.4677\n",
      "Epoch 53/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.4581\n",
      "Epoch 54/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.4487\n",
      "Epoch 55/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - loss: 0.4395\n",
      "Epoch 56/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.4304\n",
      "Epoch 57/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.4216\n",
      "Epoch 58/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.4129\n",
      "Epoch 59/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.4044\n",
      "Epoch 60/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.3961\n",
      "Epoch 61/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - loss: 0.3880\n",
      "Epoch 62/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 50ms/step - loss: 0.3800\n",
      "Epoch 63/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.3722\n",
      "Epoch 64/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - loss: 0.3646\n",
      "Epoch 65/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.3571\n",
      "Epoch 66/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 0.3497\n",
      "Epoch 67/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.3425\n",
      "Epoch 68/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.3355\n",
      "Epoch 69/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.3286\n",
      "Epoch 70/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.3219\n",
      "Epoch 71/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.3153\n",
      "Epoch 72/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 0.3088\n",
      "Epoch 73/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.3024\n",
      "Epoch 74/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.2962\n",
      "Epoch 75/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.2901\n",
      "Epoch 76/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.2842\n",
      "Epoch 77/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.2783\n",
      "Epoch 78/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - loss: 0.2726\n",
      "Epoch 79/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.2670\n",
      "Epoch 80/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.2615\n",
      "Epoch 81/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.2562\n",
      "Epoch 82/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.2509\n",
      "Epoch 83/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 0.2458\n",
      "Epoch 84/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.2407\n",
      "Epoch 85/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.2358\n",
      "Epoch 86/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.2309\n",
      "Epoch 87/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.2262\n",
      "Epoch 88/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.2215\n",
      "Epoch 89/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.2170\n",
      "Epoch 90/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.2125\n",
      "Epoch 91/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.2082\n",
      "Epoch 92/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.2039\n",
      "Epoch 93/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.1997\n",
      "Epoch 94/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.1956\n",
      "Epoch 95/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - loss: 0.1916\n",
      "Epoch 96/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.1876\n",
      "Epoch 97/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.1838\n",
      "Epoch 98/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 55ms/step - loss: 0.1800\n",
      "Epoch 99/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 0.1763\n",
      "Epoch 100/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.1727\n",
      "Epoch 101/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.1691\n",
      "Epoch 102/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.1657\n",
      "Epoch 103/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 0.1623\n",
      "Epoch 104/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.1589\n",
      "Epoch 105/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.1557\n",
      "Epoch 106/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.1525\n",
      "Epoch 107/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.1493\n",
      "Epoch 108/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.1463\n",
      "Epoch 109/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.1433\n",
      "Epoch 110/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.1403\n",
      "Epoch 111/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - loss: 0.1374\n",
      "Epoch 112/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step - loss: 0.1346\n",
      "Epoch 113/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 49ms/step - loss: 0.1319\n",
      "Epoch 114/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.1291\n",
      "Epoch 115/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.1265\n",
      "Epoch 116/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.1239\n",
      "Epoch 117/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.1213\n",
      "Epoch 118/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.1189\n",
      "Epoch 119/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.1164\n",
      "Epoch 120/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 49ms/step - loss: 0.1140\n",
      "Epoch 121/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 49ms/step - loss: 0.1117\n",
      "Epoch 122/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 0.1094\n",
      "Epoch 123/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.1071\n",
      "Epoch 124/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 50ms/step - loss: 0.1049\n",
      "Epoch 125/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.1028\n",
      "Epoch 126/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.1007\n",
      "Epoch 127/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.0986\n",
      "Epoch 128/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0966\n",
      "Epoch 129/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.0946\n",
      "Epoch 130/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.0927\n",
      "Epoch 131/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.0907\n",
      "Epoch 132/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.0889\n",
      "Epoch 133/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0871\n",
      "Epoch 134/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.0853\n",
      "Epoch 135/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0835\n",
      "Epoch 136/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0818\n",
      "Epoch 137/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.0801\n",
      "Epoch 138/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.0785\n",
      "Epoch 139/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 0.0769\n",
      "Epoch 140/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.0753\n",
      "Epoch 141/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 47ms/step - loss: 0.0737\n",
      "Epoch 142/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 0.0722\n",
      "Epoch 143/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.0707\n",
      "Epoch 144/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.0693\n",
      "Epoch 145/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0679\n",
      "Epoch 146/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.0665\n",
      "Epoch 147/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 0.0651\n",
      "Epoch 148/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step - loss: 0.0638\n",
      "Epoch 149/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0625\n",
      "Epoch 150/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0612\n",
      "Epoch 151/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.0599\n",
      "Epoch 152/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.0587\n",
      "Epoch 153/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step - loss: 0.0575\n",
      "Epoch 154/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0563\n",
      "Epoch 155/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0551\n",
      "Epoch 156/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0540\n",
      "Epoch 157/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0529\n",
      "Epoch 158/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0518\n",
      "Epoch 159/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.0508\n",
      "Epoch 160/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0497\n",
      "Epoch 161/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0487\n",
      "Epoch 162/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0477\n",
      "Epoch 163/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step - loss: 0.0467\n",
      "Epoch 164/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0458\n",
      "Epoch 165/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.0448\n",
      "Epoch 166/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0439\n",
      "Epoch 167/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0430\n",
      "Epoch 168/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.0421\n",
      "Epoch 169/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step - loss: 0.0412\n",
      "Epoch 170/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0404\n",
      "Epoch 171/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0396\n",
      "Epoch 172/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0388\n",
      "Epoch 173/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0380\n",
      "Epoch 174/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0372\n",
      "Epoch 175/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0364\n",
      "Epoch 176/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0357\n",
      "Epoch 177/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0349\n",
      "Epoch 178/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0342\n",
      "Epoch 179/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 43ms/step - loss: 0.0335\n",
      "Epoch 180/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0328\n",
      "Epoch 181/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.0321\n",
      "Epoch 182/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step - loss: 0.0315\n",
      "Epoch 183/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0308\n",
      "Epoch 184/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0302\n",
      "Epoch 185/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0296\n",
      "Epoch 186/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0290\n",
      "Epoch 187/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0284\n",
      "Epoch 188/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0278\n",
      "Epoch 189/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 45ms/step - loss: 0.0272\n",
      "Epoch 190/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0267\n",
      "Epoch 191/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0261\n",
      "Epoch 192/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.0256\n",
      "Epoch 193/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step - loss: 0.0251\n",
      "Epoch 194/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.0245\n",
      "Epoch 195/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.0240\n",
      "Epoch 196/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.0235\n",
      "Epoch 197/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.0231\n",
      "Epoch 198/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 37ms/step - loss: 0.0226\n",
      "Epoch 199/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - loss: 0.0221\n",
      "Epoch 200/200\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 41ms/step - loss: 0.0217\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 54ms/step\n",
      "[[7.]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Episode 2: Computer vision by building a neural network with TensorFlow.\n",
    "What is computer vision? Computer vision is a branch of AI that allows computers to interpret and analyze visual data like images and videos, replicating human vision capabilities. It uses techniques like machine learning and deep learning to recognize objects, detect patterns, and understand pixels. Applications include face recognition, self-driving cars, medical imaging, and augmented reality.\n",
    "When you have an image, you have to give it an id so that a computer can know which image is which but when you are doing this, you have to give it a number id instead of a string id so that there is no english bias:\n",
    "![](ImageID.png)"
   ],
   "id": "a08a9aaf661e7f36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is the code for a computer vision model that uses the mnist data set to have the computer figure out where to place what items into 1 of the 10 categories of clothing:",
   "id": "5c22f7f39f90c487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T00:56:04.233348Z",
     "start_time": "2024-12-09T00:55:46.279756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "# imports tensorflow\n",
    "mnist = tf.keras.datasets.mnist\n",
    "# loading in the mnist dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Loads in the training and testing images.\n",
    "\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "# normalize data value's between 1 and 0\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # This initializes a sequential model, which is a linear stack of layers. It allows you to define a simple feed-forward neural network by adding layers one after another.\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    #The Flatten layer reshapes the 2D input images (28x28 pixels) into a 1D array of 784 elements. This is necessary to pass the data to fully connected (dense) layers. The input_shape=(28, 28) specifies the shape of the input images.\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    #This is a fully connected (dense) layer with 128 neurons. Each neuron is connected to all neurons in the previous layer. The activation function used is ReLU (Rectified Linear Unit), if a number is less than zero it turns to zero and otherwise the number keeps it size\n",
    "\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "    #This is another dense layer with 10 neurons, corresponding to the number of output classes (e.g., digits 0-9 if working with MNIST). The softmax activation function ensures that the outputs are probabilities that sum to 1, which is ideal for multi-class classification. sets the largest to 1 and the rest to 0.\n",
    "\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# This compiles the model, specifying the optimizer and loss function.\n",
    "# The optimizer='adam' uses the Adam optimization algorithm, which combines the benefits of RMSProp and momentum-based gradient descent.\n",
    "# The loss='sparse_categorical_crossentropy' is used for multi-class classification tasks with integer-encoded labels.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "#This trains the model on the train_images and train_labels dataset for 5 epochs (iterations over the full dataset). During training, the model adjusts its weights to minimize the loss function.\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "# This evaluates the trained model on the test dataset (test_images and test_labels), returning the loss (test_loss) and accuracy (test_acc). This helps to assess how well the model generalizes to unseen data.\n",
    "\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "# prints test accuracy\n"
   ],
   "id": "fa417e35cd320be4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 2ms/step - accuracy: 0.8726 - loss: 0.4439\n",
      "Epoch 2/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9640 - loss: 0.1234\n",
      "Epoch 3/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9759 - loss: 0.0815\n",
      "Epoch 4/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9821 - loss: 0.0589\n",
      "Epoch 5/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 2ms/step - accuracy: 0.9872 - loss: 0.0442\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - accuracy: 0.9728 - loss: 0.0878\n",
      "Test accuracy: 0.9779000282287598\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Episode 3 & 4: Convolutions and Pooling:\n",
    "what is a convolution?\n",
    "A convolution is a filter that is used in machine learning and computer vision. With convolutions we are able to process a image down to its bare features that will match the labels. using many random values in filters we can find the best filter to match the image to the label to.\n",
    "what is pooling?: Pooling is a way to decrease the resolution of an image without actually damaging its integrity and makes it lighter for the program to handle and make it learn much faster.\n",
    "How does it work though?:\n",
    "Pooling: For every 2x2 pixels, it takes the one with the darkest value and it replaces the 4 pixels with a pixel of the darkest value.\n",
    "![](PoolingExplained.png)\n",
    "![](PoolingCode.png)\n",
    "Convolutions: We take 3x3 pixels from an image, then we define a 3x3 filter with specific values to multiply the images value, giving us a new image that has been filtered.\n",
    "![](ConvolutionsExplained.png)\n",
    "![](ConvolutionsCode.png)\n",
    "Now, there is one last lesson we are gonna learn, What looks Wrong in this code:\n",
    "![](26x26Code.png)\n",
    "It shows that the convolutions are 26x26 rather than 28x28? You may think that his is wrong but it is actually not, as when you are doing convolutions with an image, you can only apply this filter away from the edge so it does not mess up because of the empty parts outside of the image.\n",
    "\n",
    "Mnist Code improved using convolutions and pooling:\n",
    "\n"
   ],
   "id": "d977fece8d7820d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:16:40.604359Z",
     "start_time": "2024-12-09T21:13:26.880926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to the range 0-1\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Reshape the data to add a channel dimension (required for Conv2D)\n",
    "training_images = training_images.reshape(-1, 28, 28, 1)\n",
    "test_images = test_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    # First convolutional layer with pooling\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Second convolutional layer with pooling\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Flatten the output\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Fully connected dense layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "\n",
    "    # Output layer with 10 classes\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ],
   "id": "7c0320ca703c1dc1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 16:13:27.768530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/anton/miniforge3/envs/ml/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 15ms/step - accuracy: 0.9048 - loss: 0.3019\n",
      "Epoch 2/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 15ms/step - accuracy: 0.9871 - loss: 0.0427\n",
      "Epoch 3/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 15ms/step - accuracy: 0.9918 - loss: 0.0271\n",
      "Epoch 4/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 15ms/step - accuracy: 0.9934 - loss: 0.0191\n",
      "Epoch 5/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 16ms/step - accuracy: 0.9954 - loss: 0.0152\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.9872 - loss: 0.0463\n",
      "Test accuracy: 0.9900000095367432\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wow! Using Convolutions and Pooling that was a lot more accurate! But it did take a quiet longer and put a much bigger strain on my computer compared to the other one without pooling or convolutions.",
   "id": "97597554d346ace8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Episode 5&6: Real-world image classification using convolutional neural networks\n",
    "Ok before we build our model that deals with more complex images, we first gotta learn some new stuff:\n",
    "When we have multiple categories of images we want to store and use, how do we efficiently store all of these images into categories? simple, we create a generator:\n",
    "![](Generator.png)\n",
    "using a generator you can have tensor flow automatically name a label based off of the directory.\n",
    "Now we will examine the code for a CNN that uses a generator to tell the difference between a human and a horse:\n"
   ],
   "id": "9eb360b2bf01ec82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:43:33.580825Z",
     "start_time": "2024-12-09T21:43:33.005334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf  # TensorFlow is a library for deep learning and we’ll use it to build our model\n",
    "import os  # Provides utilities to interact with the operating system\n",
    "import zipfile  # Helps us work with ZIP files\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Prepares our images for training\n",
    "\n",
    "DESIRED_ACCURACY = 0.999  # This is the accuracy we want the model to achieve before stopping training\n",
    "\n",
    "# Download the dataset and save it locally\n",
    "!wget --no-check-certificate \\\n",
    "\"https://storage.googleapis.com/learning-datasets/happy-or-sad.zip\" \\\n",
    "-O \"/tmp/happy-or-sad.zip\"\n",
    "\n",
    "# Unzip the downloaded file into a folder so we can access the images\n",
    "zip_ref = zipfile.ZipFile(\"/tmp/happy-or-sad.zip\", 'r')  # Open the zip file\n",
    "zip_ref.extractall(\"/tmp/h-or-s\")  # Extract the contents to this folder\n",
    "zip_ref.close()  # Close the zip file\n",
    "\n",
    "# Define a custom callback that stops training once the desired accuracy is reached\n",
    "class myCallback(tf.keras.callbacks.Callback):  # We're subclassing the Callback class from TensorFlow\n",
    "    def on_epoch_end(self, epoch, logs=None):  # This runs at the end of every epoch\n",
    "        if logs.get('accuracy') >= DESIRED_ACCURACY:  # Check if the accuracy meets our threshold\n",
    "            print(\"\\nReached 99.9% accuracy so cancelling training!\")  # Print a message to notify us\n",
    "            self.model.stop_training = True  # Stop training if the condition is met\n",
    "\n",
    "callbacks = myCallback()  # Create an instance of our custom callback\n",
    "\n",
    "# Create an ImageDataGenerator to load and preprocess our images\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)  # Rescale image pixel values to range [0, 1] for normalization\n",
    "\n",
    "# Load images from the directory and prepare them for training\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/tmp/h-or-s',  # Path to the folder with training images\n",
    "    target_size=(150, 150),  # Resize all images to 150x150 pixels\n",
    "    batch_size=10,  # Load 10 images at a time for training\n",
    "    class_mode='binary'  # We’re doing binary classification (happy or sad)\n",
    ")\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([  # Sequential lets us stack layers in order\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    # A convolutional layer with 16 filters, a 3x3 kernel, ReLU activation, and input size of 150x150x3 (RGB)\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # A pooling layer that reduces image size by taking the maximum value in a 2x2 window (down-sampling)\n",
    "\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    # Another convolutional layer, but now with 32 filters for extracting more complex features\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # Pooling again to further reduce the image size while keeping important features\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    # A third convolutional layer with 64 filters for even deeper feature extraction\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # And another pooling layer to continue down-sampling the feature maps\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Flatten the output from the convolutional layers to prepare it for the dense layers\n",
    "\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # A fully connected dense layer with 512 neurons and ReLU activation to learn non-linear patterns\n",
    "\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    # The final output layer with a single neuron and sigmoid activation, because it's binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer, loss function, and evaluation metric\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  # Loss function for binary classification problems\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),  # RMSprop optimizer with a learning rate of 0.001\n",
    "    metrics=['accuracy']  # Track accuracy as the evaluation metric during training\n",
    ")\n",
    "\n",
    "# Train the model using the prepared data\n",
    "history = model.fit(\n",
    "    train_generator,  # The training data generator\n",
    "    steps_per_epoch=8,  # Total number of steps (batches) per epoch (80 images / batch size of 10)\n",
    "    epochs=20,  # Train for a maximum of 20 epochs\n",
    "    callbacks=[callbacks]  # Use our custom callback to stop training when desired accuracy is reached\n",
    ")\n"
   ],
   "id": "a8b58e31c31344eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/happy-or-sad.zip'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwget --no-check-certificate  \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://storage.googleapis.com/learning-datasets/happy-or-sad.zip\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  -\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/happy-or-sad.zip\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Unzip the downloaded file into a folder so we can access the images\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m zip_ref \u001B[38;5;241m=\u001B[39m \u001B[43mzipfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mZipFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/tmp/happy-or-sad.zip\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Open the zip file\u001B[39;00m\n\u001B[1;32m     13\u001B[0m zip_ref\u001B[38;5;241m.\u001B[39mextractall(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/h-or-s\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Extract the contents to this folder\u001B[39;00m\n\u001B[1;32m     14\u001B[0m zip_ref\u001B[38;5;241m.\u001B[39mclose()  \u001B[38;5;66;03m# Close the zip file\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/ml/lib/python3.12/zipfile/__init__.py:1331\u001B[0m, in \u001B[0;36mZipFile.__init__\u001B[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001B[0m\n\u001B[1;32m   1329\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m   1330\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1331\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp \u001B[38;5;241m=\u001B[39m \u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilemode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1332\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m   1333\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m filemode \u001B[38;5;129;01min\u001B[39;00m modeDict:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/tmp/happy-or-sad.zip'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And another example of a CNN telling the difference between a cat and a dog:",
   "id": "e2d65f61115bae01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e85bc2e2f10220e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
